$$DONE!$$
web_scraping for information:
    same as before just import into sqlite3
    take that info and push to gpread file


$$DONE!$$
build database again with sqlite3
    so that it can be used cross platform
        embed automatic push/pull to/from gspread

    tables of separate assemblies
        cols:
            id PRIMARY KEY,title,name,district,party,phone_num,email,leg_page,last,first,img_link
    tables for grades for separate assemblies
        cols:
            id PRIMARY KEY, last_updated, donations, rhetoric, voting

    use same id scheme: [assemb#][sen/rep][last[0:3]][district#]



figure out how to do it with PILLOW
    1. create config file
        - to keep it flexible
        includes:
            a. colors
            b. locations ([0,0] is top left)
            c. fonts and sizes
    2. figure out font shit

create gui:
    1. show data
    2. input/update data from manual input
    3. input/update data from spread
    4. create single/batch grade sheet
    5. on new input:
        - check for name length
        - check for exists; if exists: upadate


package it all up
    - with sqlite3, tkinter, the font/s, gspread, os, requests, bs4:BeautifulSoup, time, json, textwrap


Modules:
    spreadsheets:
        = on import: connects to sqlite3, validates data on gspread
        - format_data(data)
            - takes imported data and formats it
        - save_grades(data)
            - takes data and inserts it into db.grades
        - update_data(id, voting, rhetoric, donations)
            - takes information and updates it on gspread
        - reformat_gspread(reset_key)
            - takes reset_key as an argument. reset_key == danshaircut
            - clears the gspread workbook and inputs data from current assembly (no grades)
            - NOTE: slow because google api limits 100 calls per 100 seconds

    create_card:
        = on import: connects to sqlite3, sets constants for date
        - download_img(URL=none):
            - takes url and downloads jpg
        - create_card(id, voting, rhetoric, donation):
            - takes arguments and returns a PIL.Image object of report card
            - NOTE: does not save on local machine, only cache
        - save_card(card, directory):
            - takes PIL.Image object and directory string to save image
        - batch_create(destination)
            - takes destination directory as argument and creates report_card for all legislators
            - NOTE: (for now) destination directory MUST exist

    get_legs:
        = on import: connects to sqlite3, parses legislator table on leg.colorado.gov/legislators using bs4
        - printer(func): (depreciated)
            - wrapper function to print self.__doc__ of func
            - for easy terminal printing
            - NOTE: largely unnecessary
        - timer(func): (depreciated):
            - wrapper function to measure time
            - NOTE: largely unnecessary
        - get_leg_headers():
            - gets headers from legislator table
            - returns tuple of headers
        - get_leg_data():
            - scrapes legislator data from table
            - returns list of lists of data
        - zip_dict(key_itr, values_itr):
            - takes headings and data and zips it into a list of dictionaries
            - returns list of dictionaries of legislator data
        - split_name(data):
            - takes data as argument and splits names by first and last and appends it to data
            - returns data
        - get_portrait_link(href):
            - spider to get portrait link from each legislators bio page
            - returns None
        - append_portait_link(data):
            - takes data and for each in data appends link to portrait
            - calls get_portrait_link(href)
            - returns data
        - create_data_file():
            - main process to get data from leg.colorado.gov/legislators and save it locally in db.legislators
            - can be paired with spreadsheets.reformat_gspread() to update gspread
